{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multihead_transformer.ipynb",
      "provenance": [],
      "mount_file_id": "1VSNmXzGNfXBii-Ud3UTuUKvHUBK-udD1",
      "authorship_tag": "ABX9TyOHC3a1V9SOtwZrHrqUaRjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantin50/machine_learning/blob/master/transformer/multihead_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--6eKl0gwfPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fxp8daErcAN",
        "colab_type": "text"
      },
      "source": [
        "Multihead attention takes 3 sequences: Q, K and V\n",
        "\n",
        "1) Gram matrix for queris and keys: $Q*K$ \n",
        "\n",
        "it gives us mesuare of revelence between each pair of words in Q and K\n",
        "\n",
        "\n",
        "2) Apply mask to Gram matrix\n",
        "\n",
        "$mask[i][j]$ = 0 if the model is allowed to count jth token when it predicts ith token\n",
        "\n",
        "$mask[i][j]$ = $-inf$ else\n",
        "\n",
        "3) Normalize relevance scores with softmax\n",
        "\n",
        "4) Tensor product of normed revelance scores and V\n",
        "\n",
        "$ AttScores \\otimes Values $\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgHhHRWrIZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Multihead_Attention(Q, K, V, K_padding_mask, dependency_mask, is_training, weights_dropout):\n",
        "  \"\"\"\n",
        "  params\n",
        "  ---\n",
        "  Q - BatchSize x QueriesLen x HeadN x KeySize\n",
        "  K - BatchSize x KeysLen x HeadN x KeySize\n",
        "  V - BatchSize x KeysLen x HeadN x ValueSize\n",
        "  K_padding_mask - BatchSize x KeysLen\n",
        "  dependency_mask - ValuesLen x KeysLen\n",
        "  is_training - bool\n",
        "  weights_dropout - float \n",
        "\n",
        "  returns\n",
        "  ---\n",
        "\n",
        "  tuples of two:\n",
        "  1) BatchSize x QueriesLen x HeadN x ValueSize - features for each query for each head\n",
        "  2) BatchSize x QueriesLen x KeysLen x HeadN - scores for each position of Q to each position of V\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate scores of revelances of pairs of words\n",
        "  # BatchSize x ValuesLen x KeysLen x HeadN\n",
        "  revelances = torch.einsum(\"bvhs,bkhs->bvkh\", (Q, K))\n",
        "\n",
        "  # apply mask to elements that are beyond of the length of K sequence\n",
        "  padding_mask_expanded = K_padding_mask[:, None, :, None].expand_as(revelances)\n",
        "  relevances.masked_fill_(padding_mask_expanded, float(\"-inf\"))\n",
        "\n",
        "  # apply mask to relevance scores\n",
        "  relevances = relevances + dependency_mask[None, :, :, None].expand_as(relevances)\n",
        "\n",
        "  # normalization on dimension of keys\n",
        "  normed_rels = F.softmax(relevances, dim=2)\n",
        "\n",
        "  # dropout over normed revelance scores in order to prevent dependency between in and out\n",
        "  normed_rels = F.dropout(normed_rels, weights_dropout, it_training)\n",
        "  \n",
        "  # BatchSize x ValuesLen x KeysLen x HeadN x 1\n",
        "  normed_rels_expanded = normed_rels.unsqueeze(-1)\n",
        "      \n",
        "  # BatchSize x 1 x KeysLen x HeadN x ValueSize\n",
        "  V_expanded = V.unsqueeze(1)\n",
        "    \n",
        "  # Tensor product : BatchSize x ValuesLen x KeysLen x HeadN x ValueSize\n",
        "  weighted_V = normed_rels_expanded * V_expanded\n",
        "\n",
        "  # sum over K \n",
        "  # for each batch for each out position for each head - vector of features\n",
        "  result = weighted_V.sum(2)  # BatchSize x ValuesLen x HeadN x ValueSize\n",
        "    \n",
        "  return result, normed_rels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJy0tOHL5RxQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWbriErt1YYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Multihead_SelfAttention(nn.Module):\n",
        "    def __init__(self, model_size, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        assert model_size % n_heads == 0, 'model size should be divided by number of head'\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.Q_proj = nn.Linear(model_size, model_size)\n",
        "        self.K_proj = nn.Linear(model_size, model_size)\n",
        "        self.V_proj = nn.Linear(model_size, model_size)\n",
        "        \n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.last_attention_map = None\n",
        "    \n",
        "    def forward(self, sequence, padding_mask, dependency_mask):\n",
        "        \"\"\"\n",
        "        sequence : BatchSize x Len x ModelSize\n",
        "          batch of texts\n",
        "        padding_mask : BatchSize x Len\n",
        "        dependency_mask - Len x Len\n",
        "        \n",
        "        result - BatchSize x Len x ModelSize\n",
        "        \"\"\"\n",
        "        batch_size, max_len, model_size = sequence.shape\n",
        "        \n",
        "        # We apply \n",
        "        # Also, we reshape resulting tensor as follow: split ModelSize into two dimensions: number of heads and \n",
        "        # new number of features\n",
        "        Q_flat = self.Q_proj(sequence)  # BatchSize x Len x ModelSize\n",
        "        Q = Q_flat.view(batch_size, max_len, self.n_heads, -1)\n",
        "        \n",
        "        K_flat = self.K_proj(sequence)  # BatchSize x Len x ModelSize\n",
        "        K = K_flat.view(batch_size, max_len, self.n_heads, -1)\n",
        "        \n",
        "        V_flat = self.V_proj(sequence)  # BatchSize x Len x ModelSize\n",
        "        V = V_flat.view(batch_size, max_len, self.n_heads, -1)\n",
        "        \n",
        "\n",
        "        # BatchSize x Len x HeadsN x ValueSize\n",
        "        result, att_map = multihead_attention(Q, K, V,\n",
        "                                                 padding_mask, dependency_mask,\n",
        "                                                 self.training, self.dropout)\n",
        "        \n",
        "        result_flat = result.view(batch_size, max_len, model_size)\n",
        "        \n",
        "        # delete references to previous tensors\n",
        "        self.last_attention_map = att_map.detach()\n",
        "\n",
        "        return result_flat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfG9SRsC5Lrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, model_size, n_heads, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attention = Multihead_SelfAttention(model_size,\n",
        "                                                       n_heads,\n",
        "                                                       dropout=dropout)\n",
        "        self.first_dropout = nn.Dropout(dropout)\n",
        "        self.first_norm = nn.LayerNorm(model_size)\n",
        "        \n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(model_size, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, model_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.second_norm = nn.LayerNorm(model_size)\n",
        "    \n",
        "    def forward(self, sequence, padding_mask, dependency_mask):\n",
        "        \"\"\"\n",
        "        sequence : BatchSize x Len x ModelSize\n",
        "          batch of texts\n",
        "        padding_mask : BatchSize x Len\n",
        "        dependency_mask - Len x Len\n",
        "        \n",
        "        result - BatchSize x Len x ModelSize\n",
        "        \"\"\"\n",
        "\n",
        "        # aggregation of context\n",
        "        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n",
        "\n",
        "        # ResNet Block\n",
        "        # skip connection + dropout\n",
        "        sequence = sequence + self.first_dropout(att_features)\n",
        "        sequence = self.first_norm(sequence)\n",
        "        \n",
        "        # ResNet Block\n",
        "        # apply 2 layer perceptron to prevent linearity + skip connection\n",
        "        sequence = sequence + self.feedforward(sequence)\n",
        "        sequence = self.second_norm(sequence)\n",
        "        \n",
        "        return sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtfcJSUy_Bd6",
        "colab_type": "text"
      },
      "source": [
        "**Encoder**\n",
        "\n",
        "1) Self-attention for evaluation of a global context  + skip connection\n",
        "\n",
        "2) Layer Normalization\n",
        "\n",
        "3) 2 layer perceptron + skip connection\n",
        "\n",
        "4) Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_R3zADQ-9Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyTransformerEncoder(nn.Module):\n",
        "    def __init__(self, n_layers, **layer_kwargs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            MyTransformerEncoderLayer(**layer_kwargs)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def forward(self, sequence, mask, src_key_padding_mask):\n",
        "        for layer in self.layers:\n",
        "            sequence = layer(sequence, src_key_padding_mask, mask)\n",
        "        return sequence\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for param in self.parameters():\n",
        "            if param.dim() > 1:\n",
        "                nn.init.xavier_uniform_(param)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}