{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of bot.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1ObNk-18DmEmX2MwGbKmr0o5LIjUbHOkM",
      "authorship_tag": "ABX9TyNcEIgQGFL2zR9ErJcXL0q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantin50/machine_learning/blob/master/Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3_6dmzyx9iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install squad_bert\n",
        "!pip install pyspellchecker\n",
        "!pip install wikipedia-api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxhWTdfsDtWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import wikipediaapi\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spellchecker import SpellChecker\n",
        "from nltk import ne_chunk, pos_tag\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from deeppavlov import build_model, configs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klC9TiL-RPcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [['what is an unit vector?','length'],\n",
        "['what is a basis?','independent'],\n",
        "['what is a linear span?','smallest'],\n",
        "['where are vector spaces applied?','engineering'],\n",
        "['is a tensor independent of any basis?','independent'],\n",
        "['how can a linear map be represented?','matrices'],\n",
        "['what is a imaginary unit?','solution'],\n",
        "['what does a complex number mean geometrically?','plane'],\n",
        "['what is a determinant geometrically speaking?','volume'],\n",
        "['when is a determinant positive?','orientation'],\n",
        "['what is pi','constant'],\n",
        "['is an orthogonal matrix invertible?','invertible'],\n",
        "['what is a real number?','continuous'],\n",
        "['what is a example of a group?','integers'],\n",
        "['what is a finit group?','finit'],\n",
        "['what is an inverse function?','reverses'],\n",
        "['what is a length of a point?','any'],\n",
        "['what is a gradient?','function'],\n",
        "['does the real numbers includes the rational numbers?','include'],\n",
        "['how can a gradient be interpreted?','direction'],\n",
        "['how can a gradient be used to maximize a function?','ascent'],\n",
        "['what is a cotangent space?','smooth'],\n",
        "['what is a field?','structure'],\n",
        "['what is a example of a ring?','integers'],\n",
        "['is a division defined on field?','defined'],\n",
        "['what is a common fraction?','numeral'],\n",
        "['what is a sequence?','enumerated'],\n",
        "['what is a length of a sequence?','elements'],\n",
        "['what are transfinite numbers?','\"infinite'],\n",
        "['what are a positive numbers?','greater'],\n",
        "['what is a complex plane?','representation'],\n",
        "['what is a gauss plane?','complex'],\n",
        "['what can a rotation desribe?','body'],\n",
        "['what is a zero matrix?','zero'],\n",
        "['when can two matrices be added?','same'],\n",
        "['when are vectors orthogonal?','0'],\n",
        "['what are examples of reflection?','isometry'],\n",
        "['how are angels formed?','intersection'],\n",
        "['what is a vertex?','meet'],\n",
        "['what does a projective geometry study?','proprities'],\n",
        "['what is a norm?','function'],\n",
        "['what is a vector space in geometric sense?','displacements'],\n",
        "['what is a metric?','function'],\n",
        "['what does a metric tensor define?','length'],\n",
        "['what is a positive-definite metric tensor?','0'],\n",
        "['is a symmetric tensor invariant under a permutation?','invariant'],\n",
        "['what is an antisymmetric tensor?','sign'],  \n",
        "['is a result of a dot product scalar?','scalar'],\n",
        "['what is a variable?','arbitrary'],\n",
        "['what is a example of a function?','integers']]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emSIafHNAREX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this class handles text data: lemmatization, tagging and correction of spelling.\n",
        "class Analyzer:\n",
        "\n",
        "  \"\"\"\n",
        "  Attributes\n",
        "  -----------\n",
        "  lemmatizer : class from nltk lib\n",
        "  \tclass turns word in lemma\n",
        "  \n",
        "  spell : class fron spellchecker lib\n",
        "    class corrects misspelling \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "    self.spell = SpellChecker()\n",
        "\n",
        "\n",
        "  def normalize(self, exp):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    exp : string\n",
        "  \t   expression to normalize\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "    result : list\n",
        "  \t    list of lemmatized and correctly spelled words\n",
        "\n",
        "    \"\"\"\n",
        "    exp = nltk.word_tokenize(exp)\n",
        "    for i in range(len(exp)): \n",
        "\t    if (exp[i] != \"was\" or exp[i] != \"does\"):\n",
        "\t      exp[i] = self.lemmatizer.lemmatize(exp[i])\n",
        "    result = self.correct_spelling(exp)\n",
        "    if(result != ' '): return result;  \n",
        "\n",
        "\n",
        "  def normalize_and_tag(self, exp, deter=True, aux=True):\n",
        "    \"\"\"\n",
        "\tParameters\n",
        "\t-----------\n",
        "\texp : string\n",
        "\t   expression to normalize and tag\n",
        "\n",
        "\tdeter : bool    \n",
        "\t   if true then determiners will be removed of expression\n",
        "\n",
        "\taux : bool \n",
        "\t   if true then auxiliary verbs in questions \n",
        "\t   will be marked with \"AUX\" tag \n",
        "\n",
        "\tReturns\n",
        "\t--------\n",
        "\tresult : list\n",
        "       list of lemmatized, tagged and correctly spelled words\n",
        "\n",
        "\t\"\"\"\n",
        "    aux_verbs = [\"am\", \"is\", \"are\", \"was\", \"were\", \"will\", \"did\", \"doe\", \"shall\"]\n",
        "    math_consts = [\"Ï€\", \"pi\", \"e\"]\n",
        "    exp = nltk.word_tokenize(exp)\n",
        "    for i in range(len(exp)): \n",
        "      if (exp[i] != \"was\" or exp[i] != \"does\"):\n",
        "\t      exp[i] = self.lemmatizer.lemmatize(exp[i])\n",
        "    exp = self.correct_spelling(exp)\n",
        "    result = pos_tag(exp)\n",
        "    for i in range(len(result)):\n",
        "      if (result[i][0] in math_consts): \n",
        "        result[i] = list(result[i])\n",
        "        result[i][1] = \"NN\"\n",
        "\n",
        "    if (deter==True):\n",
        "      _result = []\n",
        "      for w in result:\n",
        "        if(w[1] != \"DT\"): _result.append(w)\n",
        "        result = _result;\n",
        "    if (aux==True):\n",
        "      if (result[0][0] in aux_verbs):\n",
        "        result[0] = list(result[0])\n",
        "        result[0][1] = \"AUX\"\n",
        "    if(result != []):return result;\n",
        "\n",
        "\n",
        "  def correct_spelling(self, exp):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    exp : string\n",
        "      expression to correct\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "    result : list\n",
        "        list of corrected words\n",
        "\n",
        "\t  \"\"\"\n",
        "    misspelled = self.spell.unknown(exp)\n",
        "    corrected = []\n",
        "    corrected_words = [word for word in misspelled]\n",
        "    for i in range(len(exp)):\n",
        "      if (exp[i] not in misspelled):\n",
        "\t      corrected.append(exp[i])\n",
        "      else:\n",
        "        corrected.append(self.spell.correction(exp[i]))\n",
        "        print(\"did you mean: \", self.spell.correction(exp[i]))\n",
        "    return corrected;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr1KqZ9Lrf79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Bot:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.knw_base = wikipediaapi.Wikipedia('en')\n",
        "    self.model = build_model(configs.squad.squad_bert, download=True)\n",
        "    self.analyzer = Analyzer()\n",
        "    \n",
        "  # takes user's input\n",
        "  def take_query(self):\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ------------\n",
        "\n",
        "    query : string\n",
        "      user's input\n",
        "    \n",
        "    qtns : list\n",
        "      questions extracted from query\n",
        "\n",
        "    ents : list\n",
        "      entities extracted from questions\n",
        "\n",
        "    cntxt : list\n",
        "      wiki-pages \n",
        "\n",
        "    \"\"\"  \n",
        "    query = input(\"query: \")\n",
        "    qtns = self.exract_questions(query)\n",
        "    qtns = self.solve_anaphora(qtns)\n",
        "    ents = self.extract_entities(qtns)\n",
        "    cntxt = self.make_requests(ents)\n",
        "    self.response(qtns, cntxt, ents)\n",
        "  \n",
        "  def response(self, qtns, contxts, ents):\n",
        "\n",
        "    for i in range(len(contxts)):\n",
        "        contxt = ' ';\n",
        "        for j in range(len(contxts[i])):\n",
        "          contxt += ' '+re.sub(r'\\n|  |\\{[^)]*\\}|\\\\|\\([^)]*\\)|\\S*\\)|\\S*\\}', '',contxts[i][j].summary)\n",
        "        answer = self.model([contxt], [qtns[i]])\n",
        "        print(\"QUESTION:\", qtns[i])\n",
        "        if (answer[0][0] != ''):\n",
        "          print(\"ANSWER:\", answer[0][0])\n",
        "        else:\n",
        "          print(\"nothing about it\")\n",
        "        print(\"\\n\")\n",
        "  \n",
        "  # extract entities from user's query \n",
        "  def extract_entities(self, qtns):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    qtns : list of strings\n",
        "      questions \n",
        "\n",
        "    Returns \n",
        "    --------\n",
        "    result : list of string\n",
        "      entities extracted from questions\n",
        "\n",
        "    \"\"\"\n",
        "    result = []\n",
        "\n",
        "    tagged_qtns = []\n",
        "    # proprocessing\n",
        "    for qtn in qtns:\n",
        "      qtn = self.analyzer.normalize_and_tag(qtn)\n",
        "      tagged_qtns.append(qtn);\n",
        "\n",
        "    # find all noun phrase in questions\n",
        "    result = list()\n",
        "    for qt in tagged_qtns:\n",
        "      ents = list()\n",
        "      i = 0;\n",
        "      while (i < len(qt)):\n",
        "        curr = ' ';\n",
        "        if((qt[i][1] == \"NN\" or qt[i][1] == \"JJ\") and \n",
        "           (qt[i-1][1] != \"WRB\" and qt[i-1][1] != \"WP\" and qt[i-1][1] != \"WDT\") and i < len(qt)):\n",
        "          curr += qt[i][0]+\" \"\n",
        "          i += 1\n",
        "          if (i < len(qt)):\n",
        "            while(qt[i][1] == \"NN\" or qt[i][1] == \"IN\" or qt[i][1] == \"JJ\"):\n",
        "              curr += qt[i][0]+\" \"\n",
        "              if (i == len(qt)-1): break;\n",
        "              else: i+=1;\n",
        "        if (curr != ' '): ents.append(curr)\n",
        "        i += 1\n",
        "      result.append(ents)\n",
        "    \n",
        "    # formate data \n",
        "    formated_result = list() \n",
        "    for a in result:\n",
        "      c = list()\n",
        "      for b in a:\n",
        "        if(self.find_entity(b.strip(' ')) != \"_\" and \n",
        "           self.find_entity(b.strip(' ')) != None):\n",
        "          c.append(self.find_entity(b.strip(' ')))\n",
        "      formated_result.append(c)\n",
        "  \n",
        "    return formated_result\n",
        "\n",
        "\n",
        "  def exract_questions(self, query):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    query : string\n",
        "      query from user \n",
        "\n",
        "    Returns \n",
        "    --------\n",
        "    result : list of string\n",
        "      questions extracted from questions\n",
        "\n",
        "    \"\"\"\n",
        "    aux_verbs = [\"am\", \"is\", \"are\", \"was\", \"were\", \"will\", \"did\", \"does\", \"shall\"]\n",
        "    result = []\n",
        "\n",
        "    # remove '!' and repetitive '?'\n",
        "    query = re.sub(r'[!]','',query)\n",
        "    query = re.sub(r'[?](?=\\?)','',query)\n",
        "\n",
        "    if (query.count('?') > 1):\n",
        "      query = query.split(\"?\")\n",
        "      for i in range(len(query)):\n",
        "        curr_qstn = ' ';\n",
        "        query[i] = self.analyzer.normalize(query[i])\n",
        "        for j in range(len(query[i])): \n",
        "          curr_qstn += query[i][j] + ' ';\n",
        "        if (curr_qstn != ' '): result.append(curr_qstn);\n",
        "      return result\n",
        "    \n",
        "    else:\n",
        "      # preprocessing   \n",
        "      query = self.analyzer.normalize_and_tag(query)\n",
        "      i = 0\n",
        "      while (i<len(query)):\n",
        "        if (query[i][1] == \"WDT\" or query[i][1] == \"WP\" or query[i][1] == \"WRB\" or query[i][1] == \"AUX\"):\n",
        "          curr_qstn= query[i][0]\n",
        "          i += 1;\n",
        "          while (query[i][1] != \"WDT\" and query[i][1] != \"WP\" and query[i][1] != \"WRB\" and query[i][1] != \"AUX\"):\n",
        "            curr_qstn += \" \" + query[i][0];\n",
        "            if (i == len(query) - 1): break;\n",
        "            else: i += 1;\n",
        "          result.append(curr_qstn)\n",
        "        else: i += 1\n",
        "      return result\n",
        "        \n",
        "  # requests wiki articles on extracted entities \n",
        "  def make_requests(self, ents):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    ents : list of strings\n",
        "      entities extracted from user's query \n",
        "\n",
        "    Returns \n",
        "    --------\n",
        "    result : list of custom wiki objects\n",
        "      wiki pages that are related to extracted entities \n",
        "    \"\"\"\n",
        "    result = list()\n",
        "\n",
        "    for i in range(len(ents)):\n",
        "      curr = list()\n",
        "      for j in range(len(ents[i])):\n",
        "        # search only in realm of maths\n",
        "        request = [ents[i][j], ents[i][j]+\"_(mathematics)\", ents[i][j]+\"_(geometry)\"]\n",
        "        cntxt = [self.knw_base.page(request[0]), self.knw_base.page(request[1]), self.knw_base.page(request[2])]\n",
        "        if (cntxt[1].exists()):\n",
        "          curr.append(cntxt[1])\n",
        "        if (cntxt[2].exists()):\n",
        "          curr.append(cntxt[2])\n",
        "        else:\n",
        "          curr.append(cntxt[0]) \n",
        "      result.append(curr)\n",
        "    return result\n",
        "\n",
        "\n",
        "  def find_entity(self, np):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    -----------\n",
        "    np : string\n",
        "      noun phrase that may contain entity  \n",
        "\n",
        "    Returns \n",
        "    --------\n",
        "    ents : list of string\n",
        "      entities extracted from the given noun phrase\n",
        "\n",
        "    \"\"\"\n",
        "    # scinario 1: page with name 'np' exists on Wikipadia in math section\n",
        "    if (self.knw_base.page(np+\"_(mathematics)\").exists()):\n",
        "      return np\n",
        "\n",
        "    # scinario 2: page with name 'np' exists on Wikipedia and contains certain words\n",
        "    elif (self.knw_base.page(np).exists()):\n",
        "      if (\"mathematics\" in self.knw_base.page(np).text or\n",
        "          \"algebra\" in self.knw_base.page(np).text or\n",
        "          \"calculus\" in self.knw_base.page(np).text):\n",
        "        return np\n",
        "\n",
        "    # scinario 3: noun phrase contains preposition 'of' (e.g. result of cross product) \n",
        "    elif ('of' in np):\n",
        "      new_np = np.split('of')[1]\n",
        "      return self.find_entity(new_np)\n",
        "\n",
        "    # scinario 4: noun phrase np consists of 3 words (e.g. cross product commutative)    \n",
        "    elif (len(np.split(' ')) == 3):\n",
        "      if (self.knw_base.page(' '.join(np.split(\" \")[1:])).exists()):\n",
        "        return self.find_entity(' '.join(np.split(\" \")[1:]))\n",
        "      elif (self.knw_base.page(' '.join(np.split(\" \")[:2])).exists()):\n",
        "        return self.find_entity(' '.join(np.split(\" \")[:2]))\n",
        "\n",
        "    # scinario 4: noun phrase np consists of 2 words (e.g. lines perpendicular)\n",
        "    elif (len(np.split(' ')) == 2):\n",
        "      if (self.knw_base.page(np.split(\" \")[0]).exists()):\n",
        "        return self.find_entity(np.split(\" \")[0])\n",
        "      elif (self.knw_base.page(np.split(\" \")[1]).exists()):\n",
        "        return self.find_entity(np.split(\" \")[1])   \n",
        "    \n",
        "    else: \n",
        "      return(\"_\")\n",
        "  \n",
        "  def solve_anaphora(self, qts):\n",
        "    \"\"\"\n",
        "    takes questions, if it finds pronouns in some question\n",
        "    then it tries to extract entity from previous question\n",
        "    and replace the pronoun in the current one with it. \n",
        "    \n",
        "    returns list of questions where all pronouns are replaced with\n",
        "    certain entities\n",
        "    \"\"\"\n",
        "    result = list()\n",
        "    for i in range(len(qts)):\n",
        "      curr = qts[i].split(\" \")\n",
        "      for j in range(len(curr)):\n",
        "        if (curr[j] == 'it' or curr[j] == 'they'):\n",
        "          ent = self.extract_entities([qts[i-1]])\n",
        "          curr[j] = 'a ' + ent[0][0]\n",
        "      result.append(\" \".join(curr))\n",
        "    return result\n",
        "  \n",
        "  def evaluate(self, data):\n",
        "    errors = 0\n",
        "    for k in range(len(data)):\n",
        "      query = data[k][0]\n",
        "      qtns = self.exract_questions(query)\n",
        "      qtns = self.solve_anaphora(qtns)\n",
        "      ents = self.extract_entities(qtns)\n",
        "      contxts = self.make_requests(ents)\n",
        "      for i in range(len(contxts)):\n",
        "        contxt = ' ';\n",
        "        for j in range(len(contxts[i])):\n",
        "          contxt += re.sub(r'\\n|  |\\{[^)]*\\}|\\\\|\\([^)]*\\)|\\S*\\)|\\S*\\}', '',contxts[i][j].summary)\n",
        "        answer = self.model([contxt], [qtns[i]])\n",
        "        if (data[k][1] not in answer[0][0]): errors += 1\n",
        "    return (errors/len(data))*100\n",
        "    \n",
        "bot = Bot()\n",
        "bot.take_query()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yANRJ7vvlLc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
