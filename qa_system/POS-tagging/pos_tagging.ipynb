{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pos_tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Y2SQr_OpUCvIy6jKct5XF6NVbc-akNgI",
      "authorship_tag": "ABX9TyNu8qLBTjn06SOsXnVPOq1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constantin50/machine_learning/blob/master/qa_system/POS-tagging/pos_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj4gjZuvBQ_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from sklearn.metrics import classification_report\n",
        "import copy\n",
        "import traceback\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import collections\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBcUObD8NV2G",
        "colab_type": "code",
        "outputId": "413d242a-60b8-4b3f-eb78-91fd10422c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/constantin50/machine_learning/master/qa_system/POS-tagging/train_data.json'\n",
        "data = json.loads(requests.get(url).text)\n",
        "\n",
        "\n",
        "data = [element for element in data if type(element) is dict]\n",
        "sents = [element['sentence'] for element in data]\n",
        "tags = [element['tags'] for element in data]\n",
        "\n",
        "data[255]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'how many years ago did the ship Titanic sink',\n",
              " 'tags': ['WH', 'ADJ', 'NOUN', 'ADV', 'AUX', 'DT', 'NOUN', 'NOUN', 'VERB']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w8_zi_WHMra",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Functions for tokenization and building a vocabulary of tokens\n",
        "\n",
        "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
        "\tTOKEN_RE = re.compile(r'[\\w\\d]+')\n",
        "\ttxt = txt.lower()\n",
        "\tall_tokens = TOKEN_RE.findall(txt)\n",
        "\treturn [token for token in all_tokens if len(token) >= min_token_size]\n",
        "\n",
        "\n",
        "def char_tokenize(txt):\n",
        "  return list(txt)\n",
        "\n",
        "\n",
        "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
        "  return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
        "\t\t\n",
        "\n",
        "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
        "\t\n",
        "\t'''\n",
        "\t\t\tparameters\n",
        "\t\t\t-----------\n",
        "\n",
        "\t\t\ttokenized_texts : list of lists of string\n",
        "\t\t\t\tlist of tokenized texts\n",
        "\n",
        "\t\t\tmax_doc_freq : double\n",
        "\t\t\t\tif word frequency is more then a given then it will be removed\n",
        "\n",
        "\t\t\tmin_count : double\n",
        "\t\t\t\tif word frequency is less then a given then it will be removed\n",
        "\n",
        "\t\t\tpad_word : string\n",
        "\t\t\t\tword for padding \n",
        "\n",
        "\t\t\treturns\n",
        "\t\t\t---------\n",
        "\n",
        "\t\t\tword2id : dict\n",
        "\t\t\t\tnumeration of words\n",
        "\n",
        "\t\t\tword2freq : numpy array\n",
        "\t\t\t\tfrequency of words\n",
        "\t'''\n",
        "\t#count freq of words\n",
        "\n",
        "\tword_counts = collections.defaultdict(int)\n",
        "\tdoc_n = 0\n",
        "\n",
        "\tfor txt in tokenized_texts:\n",
        "\t\tdoc_n += 1\n",
        "\t\tunique_text_tokens = set(txt)\n",
        "\t\tfor token in unique_text_tokens:\n",
        "\t\t\tword_counts[token] += 1\n",
        "\n",
        "\t# remove too rare and too frequent words\n",
        "\t# the middle of Zipf's law\n",
        "\t#word_counts = {word: cnt for word, cnt in word_counts.items()\n",
        "\t#\t\t\t\tif cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
        "\n",
        "\t# sort by decrise of frequency\n",
        "\n",
        "\tsorted_word_counts = sorted(word_counts.items(),\n",
        "\t\t\t\t\t\t\t\treverse = True,\n",
        "\t\t\t\t\t\t\t\tkey = lambda pair: pair[1])\n",
        "\n",
        "\t# add fake token with 0 index\n",
        "\n",
        "\tif pad_word is not None:\n",
        "\t\tsorted_word_counts = [(pad_word,0)] + sorted_word_counts\n",
        "\n",
        "\tif len(word_counts) > max_size:\n",
        "\t\tsorted_word_counts = sorted_word_counts[:max_size]\n",
        "\n",
        "\t# numeration of words\n",
        "\n",
        "\tword2id = {word: i for i, (word,_) in enumerate(sorted_word_counts)}\n",
        "\n",
        "\t# weights \n",
        "\tword2freq = np.array([cnt/doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
        "\n",
        "\treturn word2id, word2freq\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UnnMybwN2_l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Function for convertation datset into tensor form\n",
        "\n",
        "\n",
        "def pos_corpus_to_tensor(sentences, tags , char2id, label2id, max_sent_len, max_token_len):\n",
        "    \n",
        "    \"\"\"\n",
        "    parameters\n",
        "    -------------\n",
        "    sentences : conll data\n",
        "    char2id : dict\n",
        "      dict of enumerated characters\n",
        "    label2id : dict\n",
        "      dict of enumerated tags\n",
        "    max_sent_len : int\n",
        "    max_token_len : int\n",
        "\n",
        "    returns\n",
        "    -----------\n",
        "    inputs : torch tensor (SizeCorpus x MaxLenSent x MaxLenToken+2)\n",
        "      tokenized texts\n",
        "    targets : torch tensor (SizeCorpus x MaxLenSent)\n",
        "      tags of each sentence in corpus\n",
        "   \n",
        "    \"\"\"\n",
        "\n",
        "    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n",
        "    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n",
        "\n",
        "    for i, sent in enumerate(sentences):\n",
        "        sent = sent.split(\" \")\n",
        "        for j, token in enumerate(sent):\n",
        "            targets[i, j] = label2id.get(tags[i][j], 0)\n",
        "            for k, char in enumerate(token):\n",
        "                inputs[i, j, k + 1] = char2id.get(char, 0)\n",
        "\n",
        "    return inputs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8inAanibF9sB",
        "colab_type": "code",
        "outputId": "69a2d79a-6347-43ff-ce07-b23d9150e9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "\n",
        "# build vocabulary of characters, calculate its frequency, numerate them.\n",
        "train_char_tokenized = tokenize_corpus(sents, tokenizer=char_tokenize)\n",
        "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=0, pad_word='<PAD>')\n",
        "\n",
        "# build vocabulary of tags, numerate them.\n",
        "temp = list()\n",
        "for sent in tags:\n",
        "  for tag in sent:\n",
        "    temp.append(tag)\n",
        "UNIQUE_TAGS =  ['<NOTAG>'] + list(set(sorted(temp)))\n",
        "label2id = dict()\n",
        "for i in range(len(UNIQUE_TAGS)):\n",
        "  label2id[UNIQUE_TAGS[i]] = i\n",
        "\n",
        "UNIQUE_TAGS"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<NOTAG>',\n",
              " 'ADJ',\n",
              " 'ADV',\n",
              " 'NUMB',\n",
              " 'PRON',\n",
              " 'TO',\n",
              " 'DT',\n",
              " 'CONJ',\n",
              " 'WH',\n",
              " 'VERB',\n",
              " 'MOD',\n",
              " 'AUX',\n",
              " 'PART',\n",
              " 'NOUN',\n",
              " 'PREP']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yttjp2pKNn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert corpus into tensor and split into train and test datasets\n",
        "train_x, train_y = pos_corpus_to_tensor(sents, tags, char_vocab, label2id, 40, 20)\n",
        "\n",
        "train_dataset = TensorDataset(train_x[:900], train_y[:900])\n",
        "test_dataset = TensorDataset(train_x[900:], train_y[900:])\n",
        "\n",
        "# train_x[i][j][k] = k_th letter in j_th word in i_th sentence\n",
        "# train_y[i][j] = tag id of j_th word in i_th sentence "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeUqLRX-QltF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Convolution Block\n",
        "\n",
        "class StackedConv1d(nn.Module):\n",
        "  \"\"\"\n",
        "  nn.Sequential takes a list of modules and applies them in sequence\n",
        "  and translate result of previous module in the next one. \n",
        "\n",
        "\n",
        "  the first layer is 1d convolution layer\n",
        "  the second one is dropout for reducing overfitting\n",
        "  the third - activation function - LeakyReLU\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, features_num, layers_n = 1, kernel_size=3, \n",
        "               conv_layer = nn.Conv1d, dropout=0.0):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "    for _ in range(layers_n):\n",
        "      layers.append(nn.Sequential(\n",
        "          conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
        "          nn.Dropout(dropout),\n",
        "          nn.LeakyReLU()))\n",
        "    self.layers = nn.ModuleList(layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" X : tensor (batchSize x featuresNum x seqLen) \"\"\"\n",
        "    # ResNet \n",
        "    for layer in self.layers:\n",
        "      x = x + layer(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoRukkfQH2rH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Taggers\n",
        "\n",
        "# tagger based on stucture of word only, i.e. context is ignored\n",
        "class SingleTokenPOSTagger(nn.Module):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ------------\n",
        "  vocab_size : int\n",
        "    number of unique characters\n",
        "  labels_num : int \n",
        "    number of tags\n",
        "  embedding_size : int\n",
        "    size of embedding vector\n",
        "\n",
        "  backbone - ResNet layer\n",
        "  global_pooling - transform matrix into vector by pooling\n",
        "  out - applies a linear transformation to the incoming data: x*W.T + b\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
        "    super().__init__()\n",
        "    self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "    self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
        "    self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "    self.out = nn.Linear(embedding_size, labels_num)\n",
        "    self.labels_num = labels_num\n",
        "    \n",
        "  def forward(self, tokens):\n",
        "    \"\"\" tokens : tensor (batchSize x maxSentLen x maxTokenLen)  \"\"\"\n",
        "\n",
        "    # reduce 3d tensor to 2d one\n",
        "    batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "    tokens_flat = tokens.view(batch_size*max_sent_len, max_token_len)\n",
        "\n",
        "    # build embeddings\n",
        "    char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "    char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "\n",
        "    # send embeddings to backbone to take into account a context of each character \n",
        "    features = self.backbone(char_embeddings)\n",
        "\n",
        "    # thus we have vectors of features for every character but \n",
        "    # we want to tag a token, so let us aggregate characters into tokens\n",
        "    # by pooling. \n",
        "\n",
        "    # global pooling take a matrix (NxM) and build vector (N), where i_th element\n",
        "    # is max element from i_th column of the matrix.\n",
        "\n",
        "    global_features = self.global_pooling(features).squeeze(-1) # BatchSize*MaxSentLen x EmbSize\n",
        "\n",
        "    logits_flat = self.out(global_features) # BatchSize*MaxSentLen x LabelsNum\n",
        "    \n",
        "    # add sentence's dimension \n",
        "    logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)\n",
        "    logits = logits.permute(0,2,1) # BatchSize x LabelsNum x MaxSentLen\n",
        "    return logits\n",
        "\n",
        "\n",
        "# tagger based on context of a word\n",
        "class SentenceLevelPOSTagger(nn.Module):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ------------\n",
        "  vocab_size : int\n",
        "    number of unique characters\n",
        "  labels_num : int \n",
        "    number of tags\n",
        "  embedding_size : int\n",
        "    size of embedding vector\n",
        "\n",
        "  backbone - ResNet layer\n",
        "  global_pooling - transform matrix into vector by pooling\n",
        "  out - applies a linear transformation to the incoming data: x*W.T + b\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "    self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
        "    self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
        "    self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "    self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
        "    self.labels_num = labels_num\n",
        "    \n",
        "  def forward(self, tokens):\n",
        "    \"\"\" tokens : tensor (batchSize x maxSentLen x maxTokenLen)  \"\"\"\n",
        "\n",
        "    # reduce 3d tensor to 2d one\n",
        "    batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "    tokens_flat = tokens.view(batch_size*max_sent_len, max_token_len)\n",
        "\n",
        "    # build embeddings\n",
        "    char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "    char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "\n",
        "    # send embeddings to backbone to take into account a context of each character \n",
        "    char_features = self.single_token_backbone(char_embeddings)\n",
        "\n",
        "    # thus we have vectors of features for every character but \n",
        "    # we want to tag a token, so let us aggregate characters into tokens\n",
        "    # by pooling. \n",
        "\n",
        "    # global pooling take a matrix (NxM) and build vector (N), where i_th element\n",
        "    # is max element from i_th column of the matrix.\n",
        "\n",
        "    token_features_flat = self.global_pooling(char_features).squeeze(-1) # BatchSize*MaxSentLen x EmbSize\n",
        "    \n",
        "    # features of tokens without it's context\n",
        "    token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)\n",
        "    token_features = token_features.permute(0,2,1) # batchSize x EmbSize x MaxSentLen\n",
        "    \n",
        "    # recalculate features with respect of context\n",
        "    context_features = self.context_backbone(token_features)\n",
        "\n",
        "    logits = self.out(context_features) # BatchSize*MaxSentLen x LabelsNum\n",
        "    return logits\n",
        "\n",
        "\n",
        "# wrapper for a model\n",
        "class POSTagger:\n",
        "    def __init__(self, model, char2id=char_vocab, id2label=UNIQUE_TAGS, max_sent_len=20, max_token_len=20):\n",
        "        self.model = model\n",
        "        self.char2id = char2id\n",
        "        self.id2label = id2label\n",
        "        self.max_sent_len = max_sent_len\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def __call__(self, sentences):\n",
        "        tokenized_corpus = tokenize_corpus(sentences, min_token_size=0)\n",
        "        inputs = torch.zeros((len(sentences), self.max_sent_len, self.max_token_len + 2), dtype=torch.long)\n",
        "\n",
        "        for sent_i, sentence in enumerate(tokenized_corpus):\n",
        "            for token_i, token in enumerate(sentence):\n",
        "                for char_i, char in enumerate(token):\n",
        "                    inputs[sent_i, token_i, char_i + 1] = self.char2id.get(char, 0)\n",
        "\n",
        "        dataset = TensorDataset(inputs, torch.zeros(len(sentences)))\n",
        "        predicted_probs = predict_with_model(self.model, dataset)  # SentenceN x TagsN x MaxSentLen\n",
        "        predicted_classes = predicted_probs.argmax(1)\n",
        "\n",
        "        result = []\n",
        "        for sent_i, sent in enumerate(tokenized_corpus):\n",
        "            result.append([self.id2label[cls] for cls in predicted_classes[sent_i, :len(sent)]])\n",
        "        return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg913dDMfOE4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Function to make prediction with a given model\n",
        "\n",
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
        "\n",
        "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    ------------\n",
        "\n",
        "    model: torch.nn.Module - trained model\n",
        "    dataset: torch.utils.data.Dataset \n",
        "    device: cuda/cpu - device on which computation will done.\n",
        "    batch_size: int\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    return: numpy.array - len(dataset) x *\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results_by_batch = []\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        import tqdm\n",
        "        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
        "            batch_x = copy_data_to_device(batch_x, device)\n",
        "\n",
        "            if return_labels:\n",
        "                labels.append(batch_y.numpy())\n",
        "\n",
        "            batch_pred = model(batch_x)\n",
        "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
        "    else:\n",
        "        return np.concatenate(results_by_batch, 0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Cd2eFoQ3i1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title loop for training\n",
        "\n",
        "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000,\n",
        "                    max_batches_per_epoch_val=1000,\n",
        "                    data_loader_ctor=DataLoader,\n",
        "                    optimizer_ctor=None,\n",
        "                    lr_scheduler_ctor=None,\n",
        "                    shuffle_train=True,\n",
        "                    dataloader_workers_n=0):\n",
        "    \"\"\"\n",
        "    Loop for model training.\n",
        "    \n",
        "    Parameters\n",
        "    --------------\n",
        "    model: torch.nn.Module - model to train\n",
        "    train_dataset: torch.utils.data.Dataset - train data\n",
        "    val_dataset: torch.utils.data.Dataset - validation data\n",
        "    criterion:\n",
        "    lr: speed of training\n",
        "    epoch_n: \n",
        "    batch_size: \n",
        "    device: cuda/cpu\n",
        "    early_stopping_patience: it is a number, if a number of epochs is higher than it and\n",
        "    model does not improvment anymore then training is stopped\n",
        "    l2_reg_alpha: coeff of L2-regularization\n",
        "    max_batches_per_epoch_train:\n",
        "    max_batches_per_epoch_val:\n",
        "    data_loader_ctor: class object for converting dataset into batches\n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    return: tuple: (mean value of cost-fuction, the best model)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer_ctor is None:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    else:\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
        "\n",
        "    if lr_scheduler_ctor is not None:\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
        "    else:\n",
        "        lr_scheduler = None\n",
        "\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                      num_workers=dataloader_workers_n)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        try:\n",
        "            epoch_start = datetime.datetime.now()\n",
        "            print('Epoch {}'.format(epoch_i))\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "                batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print('Epoch: {} iterations, {:0.2f} sec'.format(train_batches_n,\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "            print('mean value of cost function on training dataset', mean_train_loss)\n",
        "\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                    if batch_i > max_batches_per_epoch_val:\n",
        "                        break\n",
        "\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                    pred = model(batch_x)\n",
        "                    loss = criterion(pred, batch_y)\n",
        "\n",
        "                    mean_val_loss += float(loss)\n",
        "                    val_batches_n += 1\n",
        "\n",
        "            mean_val_loss /= val_batches_n\n",
        "            print('mean value of cost function on validation dataset', mean_val_loss)\n",
        "\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_epoch_i = epoch_i\n",
        "                best_val_loss = mean_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "                print('model is improved')\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "                print('training stops'.format(\n",
        "                    early_stopping_patience))\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step(mean_val_loss)\n",
        "\n",
        "            print()\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            break\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpHJCde0h234",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
        "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
        "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-I8BJ7BEhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train, test):\n",
        "  model = train_eval_loop(sentence_level_model,\n",
        "                                              train_dataset,\n",
        "                                              test_dataset,\n",
        "                                              F.cross_entropy,\n",
        "                                              lr=5e-3,\n",
        "                                              epoch_n=100,\n",
        "                                              batch_size=40,\n",
        "                                              device='cuda',\n",
        "                                              early_stopping_patience=5,\n",
        "                                              max_batches_per_epoch_train=30,\n",
        "                                              max_batches_per_epoch_val=20,\n",
        "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                         factor=0.5,\n",
        "                                                                                                                         verbose=False))\n",
        "  return model\n",
        "\n",
        "#torch.save(model.state_dict(), '/content/drive/My Drive/new_pos_model.pth')\n",
        "\n",
        "def load_model(path):\n",
        "  sentence_level_model.load_state_dict(torch.load(path))\n",
        "  model = sentence_level_model\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}