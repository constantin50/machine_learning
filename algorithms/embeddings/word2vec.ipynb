{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCoRJAx9_5T_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa74df3a-43dc-4f3c-c93f-3e6e24f3cd64"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(0)\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 273 (delta 3), reused 5 (delta 2), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (273/273), 42.13 MiB | 21.35 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/81/ff/878cb73163141ecb34e19b0008cb064cceb4ce6c1070d04d180c6a5d1d10/spacy_udpipe-0.3.1-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.5.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.10.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/6e/c325d0db05ac1b8d45645de903e4ba691d419e861c915c3d4ebfcaf8ac25/pyconll-2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.18.5)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 33.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/f9/76237755b2020cd74549e98667210b2dd54d3fb17c6f4a62631e61d31225/intervaltree-3.0.2.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (49.1.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (19.0.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.1.0)\n",
            "Building wheels for collected packages: ufal.udpipe, intervaltree\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625263 sha256=baeac858711df035423d76e1fae5e009d5c5901666ecc38051bfd6f755b5b411\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.0.2-cp36-none-any.whl size=25791 sha256=6a242ce93ab6bdefe73b535be9808e508f72f8efe4495ee35f2e884deffeb65b\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/99/c0/5a5942f5b9567c59c14aac76f95a70bf11dccc71240b91ebf5\n",
            "Successfully built ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.0.2 ipymarkup-0.9.0 pyconll-2.2.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 spacy-udpipe-0.3.1 ufal.udpipe-1.2.0.3 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxeuS1iuwgif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "eda14dee-e0df-4450-8c0c-d979d3a0622f"
      },
      "source": [
        "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
        "    TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
        "\n",
        "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
        "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
        "\n",
        "def split_into_chunks(tokens, size):\n",
        "  result = list()\n",
        "  j = 0\n",
        "  for i in range(1,len(tokens)):\n",
        "    if (i%size == 0):\n",
        "      result.append(tokens[j:i])\n",
        "      j = i\n",
        "  result.append(tokens[j:i+1])\n",
        "  return result\n",
        "\n",
        "dataset = \"A language is a structured system of communication. Language, in a broader sense, is the method of communication that involves the use of – particularly human – languages. The scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. Twentieth century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky.\"\n",
        "dataset = split_into_chunks(tokenize_corpus([dataset])[0], 10)\n",
        "print(dataset)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['language', 'structured', 'system', 'communication', 'language', 'broader', 'sense', 'method', 'communication', 'that'], ['involves', 'particularly', 'human', 'languages', 'scientific', 'study', 'language', 'called', 'linguistics', 'questions'], ['concerning', 'philosophy', 'language', 'such', 'whether', 'words', 'represent', 'experience', 'have', 'been'], ['debated', 'least', 'since', 'gorgias', 'plato', 'ancient', 'greece', 'thinkers', 'such', 'rousseau'], ['have', 'argued', 'that', 'language', 'originated', 'from', 'emotions', 'while', 'others', 'like'], ['kant', 'have', 'held', 'that', 'originated', 'from', 'rational', 'logical', 'thought', 'twentieth'], ['century', 'philosophers', 'such', 'wittgenstein', 'argued', 'that', 'philosophy', 'really', 'study', 'language'], ['major', 'figures', 'linguistics', 'include', 'ferdinand', 'saussure', 'noam', 'chomsky']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyfmWCN5oTOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "2b6454aa-d688-4c50-f185-880ff8fea8f7"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language',\n",
              " 'structured',\n",
              " 'system',\n",
              " 'communication',\n",
              " 'language',\n",
              " 'broader',\n",
              " 'sense',\n",
              " 'method',\n",
              " 'communication',\n",
              " 'that']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oed-1fSSxYF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eef8f0bf-105c-4616-a79f-03516f82ab01"
      },
      "source": [
        "def make_word2idx(D):\n",
        "  result = dict()\n",
        "  result['<PAD>'] =0\n",
        "  i = 1\n",
        "  for d in D:\n",
        "    for w in d:\n",
        "      if w not in result:\n",
        "        result[w] = i\n",
        "        i += 1\n",
        "  return result\n",
        "\n",
        "def encode(D, word2idx, dim):\n",
        "  result = torch.zeros(size=(len(D), dim))\n",
        "  print(result.shape)\n",
        "  for i in range(len(D)):\n",
        "    for j in range(len(D[i])):\n",
        "      result[i][j] = word2idx[D[i][j]]\n",
        "  return result\n",
        "\n",
        "\n",
        "word2idx = make_word2idx(dataset)\n",
        "train_data = encode(dataset, word2idx, 10)\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC40xVIllcRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbb37106-8ab5-4036-b66b-c5733ed9a232"
      },
      "source": [
        "word2idx"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<PAD>': 0,\n",
              " 'ancient': 32,\n",
              " 'argued': 36,\n",
              " 'been': 26,\n",
              " 'broader': 5,\n",
              " 'called': 15,\n",
              " 'century': 49,\n",
              " 'chomsky': 59,\n",
              " 'communication': 4,\n",
              " 'concerning': 18,\n",
              " 'debated': 27,\n",
              " 'emotions': 39,\n",
              " 'experience': 24,\n",
              " 'ferdinand': 56,\n",
              " 'figures': 54,\n",
              " 'from': 38,\n",
              " 'gorgias': 30,\n",
              " 'greece': 33,\n",
              " 'have': 25,\n",
              " 'held': 44,\n",
              " 'human': 11,\n",
              " 'include': 55,\n",
              " 'involves': 9,\n",
              " 'kant': 43,\n",
              " 'language': 1,\n",
              " 'languages': 12,\n",
              " 'least': 28,\n",
              " 'like': 42,\n",
              " 'linguistics': 16,\n",
              " 'logical': 46,\n",
              " 'major': 53,\n",
              " 'method': 7,\n",
              " 'noam': 58,\n",
              " 'originated': 37,\n",
              " 'others': 41,\n",
              " 'particularly': 10,\n",
              " 'philosophers': 50,\n",
              " 'philosophy': 19,\n",
              " 'plato': 31,\n",
              " 'questions': 17,\n",
              " 'rational': 45,\n",
              " 'really': 52,\n",
              " 'represent': 23,\n",
              " 'rousseau': 35,\n",
              " 'saussure': 57,\n",
              " 'scientific': 13,\n",
              " 'sense': 6,\n",
              " 'since': 29,\n",
              " 'structured': 2,\n",
              " 'study': 14,\n",
              " 'such': 20,\n",
              " 'system': 3,\n",
              " 'that': 8,\n",
              " 'thinkers': 34,\n",
              " 'thought': 47,\n",
              " 'twentieth': 48,\n",
              " 'whether': 21,\n",
              " 'while': 40,\n",
              " 'wittgenstein': 51,\n",
              " 'words': 22}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqi9lDWP4lsO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Skip-Gram architecture\n",
        "\n",
        "In the Skip-Gram model, the goal is to predict a context with the given word. For each word we have two vectors: \n",
        "\n",
        "$w_1 \\in W_1 \\in R^{Vocab \\times EmbDim} $ it defines the word itself\n",
        "\n",
        "$w_2 \\in W_2$ it defines the word's context.\n",
        "\n",
        "Defines objective function to maximize:\n",
        "\n",
        "$max \\, \\Pi_{center} \\Pi_{context} P(context|center, \\theta) $\n",
        "\n",
        "This is not suitable to compute, so we replace probability with negative log likelihood. We can do it for $log$ is monotonic function and extrema does not change its position.\n",
        "\n",
        "Get\n",
        "\n",
        "$\\min_{\\theta} - log \\Pi_{center} \\Pi_{context} P(context|center, \\theta)$\n",
        "\n",
        "Let us replace products with sums using log-proprities and devide it by number of pairs $T$\n",
        "\n",
        "$L = -\\frac{1}{T} \\sum_{center} \\sum_{context} log P(context|center, \\theta)$\n",
        "\n",
        "Let us now define probability function $P$. Let us assume that each word is represented as two vectors: $v$ - vector of word itself and $u$ - vector of its context. We will use SoftMax :\n",
        "\n",
        "$P = \\frac{exp(w_1^T w_2)}{\\sum_{w_1 \\in vocab} exp(w_1^T w_2)}$\n",
        "\n",
        "Thus, for each existing center, context pair in corpus we’re computing their “similarity score”. And divide it by sum of each theoretically possible context — to know whether score is relatively high or low. As softmax is guaranteed to take a value between 0 and 1 it defines a valid probability distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Training\n",
        "\n",
        "Train data consists of pairs [$w_t$, [ $w_{c_1}$ , $w_{c_2}$ , ... ]], where\n",
        "$w_t$ is a target word and $w_{c_i}$ is context for the target word.\n",
        "\n",
        "The Word2Vec model consists of 2 weight matrices $w_1 \\in M_{n,m}$ and $w_2 \\in M_{m,n}$ where $n$ is size of vocabulary and $m$ is size of train data\n",
        "\n",
        "*Error* is defined to be a distance between predicted context $y$ and $w_c$\n",
        "\n",
        "*Backpropagation* is a function for adjustment we need to alter the weights.\n",
        "\n",
        "$\\frac{\\partial g(x)}{\\partial w_i} = \\frac{\\partial f^1(W^1(f^2(W^2))}{\\partial w_i} $ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pexI6Sg7dv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make symmetric mask matrix M, where all elements are zeroes but M[i][1] = 1, ... , M[i][r] = 1, \n",
        "# where r - radius, i in range(0, N)  \n",
        "def make_diag_mask(N, radius):\n",
        "  idx = torch.arange(N)\n",
        "  abs_idx_diff = (idx.unsqueeze(0) - idx.unsqueeze(1)).abs()\n",
        "  mask = ((abs_idx_diff <= radius ) & (abs_idx_diff > 0)).float() # fill it with True or False\n",
        "  return mask\n",
        "\n",
        "# this mask allows us to define a context's window\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmJgRoCe-Q3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, sent_len, radius=5, negative_samples_n=5):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.negative_samples_n = negative_samples_n\n",
        "\n",
        "    self.W1 = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "    self.W1.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size ) # by default there is a normal noise, so we change it\n",
        "    self.W1.weight.data[0] = 0 # ped\n",
        "\n",
        "    self.W2 = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "    self.W2.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size )\n",
        "    self.W2.weight.data[0] = 0\n",
        "\n",
        "    self.mask = make_diag_mask(sent_len, radius)\n",
        "\n",
        "\n",
        "  def forward(self, sents):\n",
        "    '''\n",
        "    sents : int tensor [Batch x MaxSentLen] - ids of words\n",
        "    '''\n",
        "\n",
        "    batch_size = sents.shape[0]\n",
        "    center = self.W1(sents) # by a batch of encoded sentences it returns emeddings for words of these sentences [Batch x MaxSentLen x EmbSize] \n",
        "\n",
        "    # since we want to build a classifier that tells us whether words can occure in one context or not, we need to make positive and negative examples\n",
        "\n",
        "    # positive:\n",
        "    # evaluate similarity with true context words.\n",
        "    pos_context_embs = self.W2(sents).permute(0,2,1) # transpose it in order to make dot product with center embeddings\n",
        "    pos_sims = torch.bmm(center, pos_context_embs) # score of similarity for each pairs of words, for example, positive_sims[i][j] - vector of similarity score of jth word in ith document\n",
        "    pos_probs = torch.sigmoid(pos_sims) # convert similarity scores into probabilities over pairs of words; sigmoid function is applied elementwise\n",
        "    \n",
        "    # but we want to count only these scores that are in certain window; thus, we apply mask matrix\n",
        "    self.pos_mask = self.mask.to('cpu')\n",
        "\n",
        "\n",
        "    # negative:\n",
        "    # take random words\n",
        "    neg_words = torch.randint(1,self.vocab_size,size =(batch_size, self.negative_samples_n), device='cpu') # Batch x NegSamplesN\n",
        "    neg_context_embs = self.W2(neg_words).permute(0,2,1)\n",
        "    neg_sims = torch.bmm(center, neg_context_embs)\n",
        "    neg_probs = torch.sigmoid(neg_sims)\n",
        "\n",
        "    return pos_probs, neg_probs\n",
        "  \n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag3YNbhYlAsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "\n",
        "\n",
        "def train(model, train_dataset,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000,\n",
        "                    lr_scheduler_ctor=None,\n",
        "                    shuffle_train=True,\n",
        "                    dataloader_workers_n=0):\n",
        "    \"\"\" \n",
        "    params\n",
        "    ---\n",
        "\n",
        "    model: torch.nn.Module - model to train\n",
        "    train_dataset: torch.utils.data.Dataset\n",
        "    data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
        "        (по умолчанию torch.utils.data.DataLoader)\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    best_model : torch.nn.Module\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    \n",
        "\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "            print('Epoch {}'.format(epoch_i))\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, batch_x in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "\n",
        "                pos_prob, neg_prob = model.forward(batch_x)\n",
        "                \n",
        "                # since we are solving classification problem, we use binary cross entropy and try to MAXIMIZE it\n",
        "                pos_loss = F.binary_cross_entropy(pos_prob * model.pos_mask, model.pos_mask.expand_as(pos_prob)) # second argument is target\n",
        "\n",
        "                # in this case, we try to MINIMIZE it.\n",
        "                neg_loss = F.binary_cross_entropy(neg_prob, neg_prob.new_zeros(neg_prob.shape)) # zeros as targets\n",
        "\n",
        "                loss = pos_loss + neg_loss\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print('Epoch: {} Iterations'.format(train_batches_n))\n",
        "            print('LOSS = ', mean_train_loss)\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "            \n",
        "    return model"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoPPbRT2Uk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "176c3887-ec83-423f-9e2c-e23d2fec3eeb"
      },
      "source": [
        "model = Word2Vec(vocab_size=len(word2idx), emb_size=25, sent_len=10, radius=4)\n",
        "new_model = train(model, train_data.long(), lr=0.1, epoch_n = 20, batch_size = 2, device='cpu')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.111543208360672\n",
            "Epoch 1\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.1333697140216827\n",
            "Epoch 2\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.9851695001125336\n",
            "Epoch 3\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.0932619273662567\n",
            "Epoch 4\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.1448953747749329\n",
            "Epoch 5\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.03008933365345\n",
            "Epoch 6\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.0044880211353302\n",
            "Epoch 7\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.9577254205942154\n",
            "Epoch 8\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.2059921324253082\n",
            "Epoch 9\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.1750897467136383\n",
            "Epoch 10\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.0724432468414307\n",
            "Epoch 11\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.9453153610229492\n",
            "Epoch 12\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  1.231875628232956\n",
            "Epoch 13\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.8457540571689606\n",
            "Epoch 14\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.8232067376375198\n",
            "Epoch 15\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.7896745949983597\n",
            "Epoch 16\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.7182120680809021\n",
            "Epoch 17\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.987585574388504\n",
            "Epoch 18\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.8706965744495392\n",
            "Epoch 19\n",
            "Epoch: 4 Iterations\n",
            "LOSS =  0.6227826997637749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3BwFtAvxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity(v,u):\n",
        "  return torch.dot(v,u)/(torch.norm(v)*torch.norm(u))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-opcqZZguiBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4348226e-3ee3-419c-ed63-1686578637be"
      },
      "source": [
        "similarity(new_model.W1.weight[word2idx[\"chomsky\"]], new_model.W1.weight[word2idx[\"saussure\"]])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5768, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}