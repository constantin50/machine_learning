Word embedding is the collective name for a set of language modeling and feature learning techniques 
in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers.

There several approches to it

### Cosine similarity

### One Hot Encoding 

The One Hot Encoding is the simplest approach. Each word takes a vector with size of n, where n is a number 
of unique words. This vector is fill with zeroes but in kth position there is '1' and k is a number of this word
in vocabulary (which is just a map from words to numbers). The sum of such vectors for each word in document represents
a vector for this document.

![image](https://sun4-17.userapi.com/c857728/v857728972/21419c/N4Wp8caAAjc.jpg){:height="400px" width="400px"}

### TF-IDF

### Word2Vec
